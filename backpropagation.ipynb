{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i : lambda x : (1 / (torch.cosh(x)**2)) \n",
    "            for i in range(1, self.L+1)\n",
    "        }\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    print(\"y_true = \", y_true)\n",
    "    print(\"y_pred = \", y_pred)\n",
    "\n",
    "    # It might make sense to move this propert into the model class\n",
    "    # That way we can change it along with the weights if we prefer.\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    #\n",
    "    # Compute slopes\n",
    "    #\n",
    "\n",
    "    # Each slope computation requires the slope from the next layer\n",
    "    # Therefore, we compute the slope for the output layer first, then work backwards\n",
    "\n",
    "    output_error = nn.MSELoss(reduction=\"none\")(y_pred, y_true)\n",
    "    output_slope = output_error * model.df[model.L](y_pred)\n",
    "\n",
    "    model.dL_dw[model.L] = output_slope\n",
    "    model.dL_db[model.L] = output_slope\n",
    "\n",
    "    # Compute slopes for hidden layers\n",
    "    for layer in reversed(range(1, model.L)):\n",
    "\n",
    "        # Calculate error\n",
    "        next_slope = model.dL_dw[layer + 1]\n",
    "        next_out = model.a[layer + 1]\n",
    "        next_out_transposed = torch.transpose(next_out, 0, 1)\n",
    "\n",
    "        error = torch.matmul(next_slope, next_out_transposed)\n",
    "\n",
    "        # Calculate slope\n",
    "        out = model.a[layer]\n",
    "        slope = error * model.df[layer](out)\n",
    "        \n",
    "        # Store slope\n",
    "        model.dL_dw[layer] = slope\n",
    "        model.dL_db[layer] = slope\n",
    "\n",
    "    #\n",
    "    # Update weights and biases\n",
    "    #\n",
    "\n",
    "    # The first layer is the input layer and does not have weights not biases.\n",
    "    # Therefore the first updates layer has index 1 (0 being the first)\n",
    "\n",
    "    return\n",
    "\n",
    "    for layer in reversed(range(1, model.L - 1)):\n",
    "\n",
    "        # Compute update for weights\n",
    "        previous_out = model.a[layer - 1]\n",
    "        previous_out_transposed = torch.transpose(previous_out, 0, 1)\n",
    "        weight_slope = model.dL_dw[layer]\n",
    "        weight_change = torch.matmul(previous_out_transposed, weight_slope) * learning_rate\n",
    "        \n",
    "        # Compute update for biases\n",
    "        bias_slope = model.dL_db[layer]\n",
    "        bias_change = bias_slope.sum() * learning_rate\n",
    "\n",
    "        # Apply updates\n",
    "        parameters = model.fc[str(layer)]\n",
    "        parameters.weight.add_(weight_change)\n",
    "        parameters.bias.add_(bias_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "y_true =  tensor([[0., 0.]])\n",
      "y_pred =  tensor([[-0.3391,  0.4280]], grad_fn=<TanhBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MyNet([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackpropagation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mats\\Dropbox\\Mats\\uib\\24H1\\INF265\\project1\\tests_backpropagation.py:271\u001b[0m, in \u001b[0;36mmain_test\u001b[1;34m(backprop_fn, model, eps, verbose, data)\u001b[0m\n\u001b[0;32m    269\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Training loop that compares our gradients with autograd's computations\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m autograd_ok, gradcheck_ok \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackprop_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m autograd_ok:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m TEST PASSED: Gradients consistent with autograd\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms computations.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mats\\Dropbox\\Mats\\uib\\24H1\\INF265\\project1\\tests_backpropagation.py:212\u001b[0m, in \u001b[0;36mcheck_gradients\u001b[1;34m(model, backprop_fn, optimizer, loss_fn, x, y_true, n_epochs, eps, verbose)\u001b[0m\n\u001b[0;32m    209\u001b[0m loss\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    211\u001b[0m backprop_fn(model, out_expected, out)\n\u001b[1;32m--> 212\u001b[0m flag, res \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_expected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m gradcheck_ok \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m flag\n\u001b[0;32m    214\u001b[0m res_backprop\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mround\u001b[39m(res\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Mats\\Dropbox\\Mats\\uib\\24H1\\INF265\\project1\\tests_backpropagation.py:104\u001b[0m, in \u001b[0;36mgrad_check\u001b[1;34m(model, x, y, loss_fn, eps)\u001b[0m\n\u001b[0;32m    102\u001b[0m                     grad_backprop\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mdL_db[i_layer][idx])\n\u001b[0;32m    103\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m                     grad_backprop\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdL_dw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    106\u001b[0m g_approx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(grad_approx)\n\u001b[0;32m    107\u001b[0m g_backprop \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(grad_backprop)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=True, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\raw\\train-images-idx3-ubyte.gz to ../data/MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "31.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "y_true =  tensor([[7.]])\n",
      "y_pred =  tensor([[ 0.1090, -0.3560,  0.5218, -0.2193,  0.2857,  0.2657, -0.0566,  0.4653,\n",
      "         -0.4371,  0.2910]], grad_fn=<TanhBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mats\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for dimension 1 with size 16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MyNet([\u001b[38;5;241m24\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackpropagation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mats\\Dropbox\\Mats\\uib\\24H1\\INF265\\project1\\tests_backpropagation.py:271\u001b[0m, in \u001b[0;36mmain_test\u001b[1;34m(backprop_fn, model, eps, verbose, data)\u001b[0m\n\u001b[0;32m    269\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Training loop that compares our gradients with autograd's computations\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m autograd_ok, gradcheck_ok \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackprop_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m autograd_ok:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m TEST PASSED: Gradients consistent with autograd\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms computations.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mats\\Dropbox\\Mats\\uib\\24H1\\INF265\\project1\\tests_backpropagation.py:212\u001b[0m, in \u001b[0;36mcheck_gradients\u001b[1;34m(model, backprop_fn, optimizer, loss_fn, x, y_true, n_epochs, eps, verbose)\u001b[0m\n\u001b[0;32m    209\u001b[0m loss\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    211\u001b[0m backprop_fn(model, out_expected, out)\n\u001b[1;32m--> 212\u001b[0m flag, res \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_expected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m gradcheck_ok \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m flag\n\u001b[0;32m    214\u001b[0m res_backprop\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mround\u001b[39m(res\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Mats\\Dropbox\\Mats\\uib\\24H1\\INF265\\project1\\tests_backpropagation.py:104\u001b[0m, in \u001b[0;36mgrad_check\u001b[1;34m(model, x, y, loss_fn, eps)\u001b[0m\n\u001b[0;32m    102\u001b[0m                     grad_backprop\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mdL_db[i_layer][idx])\n\u001b[0;32m    103\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m                     grad_backprop\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdL_dw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    106\u001b[0m g_approx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(grad_approx)\n\u001b[0;32m    107\u001b[0m g_backprop \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(grad_backprop)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 16 is out of bounds for dimension 1 with size 16"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=True, data='mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nglm-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ae36e8c2cbd9e14d80419493f2540eab6c211be174ac39ce04705a74740d0d8b"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
