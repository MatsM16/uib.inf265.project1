{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.1151, 0.2688, 0.4053, 0.3931, 0.1285, 0.0924, 0.155, 0.0438, 0.156, 0.0461]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-1.7814e-05, -1.7557e-05],\n",
      "        [-5.6907e-05, -5.6087e-05],\n",
      "        [ 1.5985e-04,  1.5754e-04]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[-1.7763e-05, -1.7507e-05],\n",
      "        [-5.6887e-05, -5.6067e-05],\n",
      "        [ 1.5984e-04,  1.5753e-04]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-1.9551e-06, -6.2455e-06,  1.7543e-05])\n",
      "  Autograd's computation:\n",
      " tensor([-1.9495e-06, -6.2433e-06,  1.7542e-05])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0003\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0003\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 2 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0177, 0.0033, 0.0441, 0.042, 0.0455, 0.1259, 0.1204, 0.1519, 0.1737, 0.4031]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-9.6296e-06, -9.4907e-06],\n",
      "        [-2.5898e-05, -2.5525e-05],\n",
      "        [ 4.9429e-05,  4.8716e-05]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[-9.6530e-06, -9.5138e-06],\n",
      "        [-2.5867e-05, -2.5494e-05],\n",
      "        [ 4.9460e-05,  4.8746e-05]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-1.0568e-06, -2.8423e-06,  5.4247e-06])\n",
      "  Autograd's computation:\n",
      " tensor([-1.0594e-06, -2.8388e-06,  5.4281e-06])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0009\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0009\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 3 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0217, 0.007, 0.039, 0.0618, 0.1167, 0.1908, 0.2823, 0.0091, 0.0511, 0.3854]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-6.8283e-06, -6.7298e-06],\n",
      "        [-1.6320e-05, -1.6085e-05],\n",
      "        [ 2.6196e-05,  2.5818e-05]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[-6.8562e-06, -6.7574e-06],\n",
      "        [-1.6304e-05, -1.6069e-05],\n",
      "        [ 2.6169e-05,  2.5791e-05]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-7.4939e-07, -1.7911e-06,  2.8750e-06])\n",
      "  Autograd's computation:\n",
      " tensor([-7.5246e-07, -1.7893e-06,  2.8720e-06])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0013\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0013\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 4 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0182, 0.0115, 0.0449, 0.1506, 0.2459, 0.0667, 0.3473, 0.3894, 1.0, 0.4419]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-5.3717e-06, -5.2943e-06],\n",
      "        [-1.1723e-05, -1.1554e-05],\n",
      "        [ 1.6939e-05,  1.6695e-05]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[-5.3784e-06, -5.3009e-06],\n",
      "        [-1.1730e-05, -1.1561e-05],\n",
      "        [ 1.6948e-05,  1.6703e-05]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-5.8954e-07, -1.2866e-06,  1.8590e-06])\n",
      "  Autograd's computation:\n",
      " tensor([-5.9027e-07, -1.2873e-06,  1.8600e-06])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0006\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0006\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " ====================== Epoch 5 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0183, 0.0082, 0.0569, 0.2196, 0.1215, 0.1016, 1.0, 0.2689, 0.51, 0.4931]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[-4.4681e-06, -4.4037e-06],\n",
      "        [-9.0507e-06, -8.9202e-06],\n",
      "        [ 1.2162e-05,  1.1986e-05]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[-4.4529e-06, -4.3887e-06],\n",
      "        [-9.0638e-06, -8.9331e-06],\n",
      "        [ 1.2168e-05,  1.1993e-05]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-4.9036e-07, -9.9330e-07,  1.3347e-06])\n",
      "  Autograd's computation:\n",
      " tensor([-4.8870e-07, -9.9474e-07,  1.3354e-06])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0013\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0013\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with autograd's computations.\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "tensor([[ 0.5159,  0.1609],\n",
      "        [ 0.0351,  0.5570],\n",
      "        [-0.2160, -0.4580]])\n",
      "tensor([0.3013, 0.6066, 0.2667])\n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 2 / 4: SOME TESTS FAILED, use 'verbose=True' and check the output for more details\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i : lambda x : (1 / (torch.cosh(x)**2)) \n",
    "            for i in range(1, self.L+1)\n",
    "        }\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    with torch.no_grad():\n",
    "        # MSE = (pred - true)^2\n",
    "        # MSE_derived = 2(pred - true)\n",
    "        # The slope of the loss function can be computed like this:\n",
    "        grad_post_activation = 2 * (y_pred - y_true)\n",
    "\n",
    "        # Compute gradient for each layer backwords from output layer to the first hidden layer\n",
    "        for layer in range(model.L, 0, -1):\n",
    "\n",
    "            # Slope of loss with respect to pre-activation of current layer\n",
    "            grad_pre_activation = grad_post_activation * model.df[layer](model.z[layer])\n",
    "\n",
    "            # Compute gradients of the loss with respect to the weights and biases of the current layer\n",
    "            model.dL_dw[layer] = torch.matmul(grad_pre_activation.T, model.a[layer - 1])\n",
    "            model.dL_db[layer] = torch.sum(grad_pre_activation, dim=0)\n",
    "\n",
    "            # Compute the gradient of the loss with respect to the output of the previous layer\n",
    "            grad_post_activation = torch.matmul(grad_pre_activation, model.fc[str(layer)].weight)\n",
    "\n",
    "        learning_rate = 1e-9\n",
    "        for layer in range(1, model.L + 1):\n",
    "            model.fc[str(layer)].weight -= learning_rate * model.dL_dw[layer]\n",
    "            model.fc[str(layer)].bias -= learning_rate * model.dL_db[layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=False, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=False, data='mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nglm-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ae36e8c2cbd9e14d80419493f2540eab6c211be174ac39ce04705a74740d0d8b"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
