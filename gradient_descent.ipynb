{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "TRAIN_SPLIT_PERCENTAGE = 0.8\n",
    "VALIDATION_SPLIT_PERCENTAGE = 1 - TRAIN_SPLIT_PERCENTAGE\n",
    "RANDOM_SEED = 265\n",
    "EPOCH_COUNT = 30\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.ConvertImageDtype(torch.double),\n",
    "\n",
    "    # Normalize the pixel color values to be between -1 and 1\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dataset to 'airplane' and 'bird' only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "Included labels:  ['airplane', 'bird']\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_LABELS = [label for label in cifar10_test.classes]\n",
    "INCLUDED_LABELS = ['airplane', 'bird']\n",
    "\n",
    "print(\"Labels: \", CIFAR10_LABELS)\n",
    "print(\"Included labels: \", INCLUDED_LABELS)\n",
    "\n",
    "included_labels_indices = [i for i, label in enumerate(CIFAR10_LABELS) if label in INCLUDED_LABELS]\n",
    "\n",
    "cifar10_train_included_indices = [i for i, target in enumerate(cifar10_train.targets) if target in included_labels_indices]\n",
    "cifar10_test_included_indices = [i for i, target in enumerate(cifar10_test.targets) if target in included_labels_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and create loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8000 entries\n",
      "Validate: 2000 entries\n",
      "Test: 2000 entries\n"
     ]
    }
   ],
   "source": [
    "train_indices = random.sample(cifar10_train_included_indices, int(TRAIN_SPLIT_PERCENTAGE * len(cifar10_train_included_indices)))\n",
    "val_indices = [i for i in cifar10_train_included_indices if i not in train_indices]\n",
    "\n",
    "train_subset = torch.utils.data.Subset(cifar10_train, train_indices)\n",
    "val_subset = torch.utils.data.Subset(cifar10_train, val_indices)\n",
    "test_subset = torch.utils.data.Subset(cifar10_test, cifar10_test_included_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train: {len(train_subset)} entries\")\n",
    "print(f\"Validate: {len(val_indices)} entries\")\n",
    "print(f\"Test: {len(cifar10_test_included_indices)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the dataset\n",
    "I will now visualize and analyse the dataset.\n",
    "\n",
    "## Obvious bises?\n",
    "To identify obvious biases in the dataset, I need to visualize the distribution of labels.  \n",
    "If one label is grosely overrepresented, I might have to downsample the other label.\n",
    "\n",
    "## What does the data look like?\n",
    "The labels sais birds and planes, but I want to know excactly what I am working with.  \n",
    "Therefore I want to show one example of each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_data_example(image_tensor):\n",
    "    # Unnormalize the pixel color values\n",
    "    image_tensor = image_tensor * 0.5 + 0.5\n",
    "\n",
    "    np_image = image_tensor.numpy()\n",
    "    np_image = np.transpose(np_image, (1, 2, 0))\n",
    "    plt.imshow(np_image)\n",
    "    plt.show()\n",
    "\n",
    "def show_data_examples(classes, loader):\n",
    "    include_labels = [x for x in classes]\n",
    "    examples = []\n",
    "    for images, labels in loader:\n",
    "        if len(include_labels) == 0: continue\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            label = classes[labels[i].item()]\n",
    "            if label not in include_labels: continue\n",
    "            examples.append(images[i])\n",
    "            include_labels.remove(label)\n",
    "\n",
    "    show_data_example(torchvision.utils.make_grid(examples))\n",
    "\n",
    "def show_label_distribution(classes, loader):\n",
    "    distribution = {label: 0 for label in classes}\n",
    "    for _, labels in loader:\n",
    "        for label in labels:\n",
    "            label_name = classes[label.item()]\n",
    "            distribution[label_name] += 1\n",
    "\n",
    "    plt.bar(distribution.keys(), distribution.values())\n",
    "    plt.show()\n",
    "\n",
    "# show_label_distribution(included_labels, train_loader)\n",
    "# show_data_examples(included_labels, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.input = nn.Linear(3 * 32 * 32, 512)\n",
    "        self.hidden1 = nn.Linear(512, 128)\n",
    "        self.hidden2 = nn.Linear(128, 32)\n",
    "        self.hidden3 = nn.Linear(32, 2)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.activation(self.input(x))\n",
    "        x = self.activation(self.hidden1(x))\n",
    "        x = self.activation(self.hidden2(x))\n",
    "        x = self.hidden3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(epoch_count, optimizer, model, loss_function, train_loader, eval_mid_training=False):\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "\n",
    "        # We aggregate the loss to print after each epoch\n",
    "        # This is only for debugging purposes\n",
    "        epoch_loss = 0.0\n",
    "        epoch_size = 0\n",
    "\n",
    "        for X, y_true in train_loader:\n",
    "\n",
    "            # Gradient descent\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # Map the labels to neurons\n",
    "            # Label 0 (airplane) -> Neuron 0\n",
    "            # Label 2 (bird) -> Neuron 1\n",
    "            y_true = torch.Tensor([[1.0, 0.0] if i.item() == 0 else [0.0, 1.0] for i in y_true])\n",
    "\n",
    "            loss = loss_function(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_size += 1\n",
    "\n",
    "        # Print loss after some epochs\n",
    "        # This is for debugging purposes\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch + 1} | Training loss: {epoch_loss / epoch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y_true in test_loader:\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # Pick the class with the highest probability\n",
    "            value, y_pred = torch.max(y_pred.data, 1)\n",
    "            \n",
    "            # Map the predicted neurons to labels:\n",
    "            # Neuron 0 -> Label 0 (airplane)\n",
    "            # Neuron 1 = Label 2 (bird)\n",
    "            y_pred = torch.tensor([CIFAR10_LABELS.index(INCLUDED_LABELS[i.item()]) for i in y_pred])\n",
    "\n",
    "            # Update the score\n",
    "            total += y_true.size(0)\n",
    "            correct += (y_pred == y_true).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def train_and_eval(learning_rate, momentum, weight_decay):\n",
    "    print(\"=========================================================\")\n",
    "    print(\"Current parameters:\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Momentum: {momentum}\")\n",
    "    print(f\"Weight decay: {weight_decay}\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"--------- Using Pytorch's SGD ---------\")\n",
    "    model_sgd = MyMLP()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model_sgd.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    train(EPOCH_COUNT, optimizer, model_sgd, nn.CrossEntropyLoss(), train_loader)\n",
    "\n",
    "    accuracy_sgd = compute_accuracy(model_sgd, val_loader)\n",
    "    print(\"\")\n",
    "    print(\"--- Accuracies ---\")\n",
    "    print(\"Training accuracy: \", compute_accuracy(model_sgd, train_loader))\n",
    "    print(\"Validation accuracy: \", accuracy_sgd)\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"--------- Using manual update ----------\")\n",
    "    model_manual = MyMLP()\n",
    "\n",
    "    # Train somehow\n",
    "\n",
    "    accuracy_manual = compute_accuracy(model_manual, val_loader)\n",
    "    print(\"\")\n",
    "    print(\"--- Accuracies ---\")\n",
    "    print(\"Training accuracy: \", compute_accuracy(model_manual, train_loader))\n",
    "    print(\"Validation accuracy: \", accuracy_manual)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global parameters:\n",
      "Batch size:  256\n",
      "Epoch count:  30\n",
      "Loss function: CrossEntropyLoss\n",
      "Seed:  265\n",
      "=========================================================\n",
      "Current parameters:\n",
      "Learning rate: 0.01\n",
      "Momentum: 0.0\n",
      "Weight decay: 0.0\n",
      "\n",
      "--------- Using Pytorch's SGD ---------\n",
      "Epoch 1 | Training loss: 0.6898269883174096\n",
      "Epoch 6 | Training loss: 0.6109174819521347\n",
      "Epoch 11 | Training loss: 0.5175541367533308\n",
      "Epoch 16 | Training loss: 0.47256448233649034\n",
      "Epoch 21 | Training loss: 0.4485086135876281\n",
      "Epoch 26 | Training loss: 0.4271800653296917\n",
      "\n",
      "--- Accuracies ---\n",
      "Training accuracy:  0.833\n",
      "Validation accuracy:  0.803\n",
      "\n",
      "--------- Using manual update ----------\n",
      "Epoch 1 | Training loss: 0.4028866515981736\n",
      "Epoch 6 | Training loss: 0.38320606981146677\n",
      "Epoch 11 | Training loss: 0.3676281693166787\n",
      "Epoch 16 | Training loss: 0.3466304459065107\n",
      "Epoch 21 | Training loss: 0.32550423644263127\n",
      "Epoch 26 | Training loss: 0.30464056528421435\n",
      "=========================================================\n",
      "Current parameters:\n",
      "Learning rate: 0.01\n",
      "Momentum: 0.0\n",
      "Weight decay: 0.01\n",
      "\n",
      "--------- Using Pytorch's SGD ---------\n",
      "Epoch 1 | Training loss: 0.6861913189229112\n",
      "Epoch 6 | Training loss: 0.6217627066081013\n",
      "Epoch 11 | Training loss: 0.539713303731373\n",
      "Epoch 16 | Training loss: 0.48719777052519275\n",
      "Epoch 21 | Training loss: 0.4624377725498832\n",
      "Epoch 26 | Training loss: 0.4383119645847375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m hyper_parameters:\n\u001b[0;32m     21\u001b[0m     model \u001b[38;5;241m=\u001b[39m MyMLP()\n\u001b[1;32m---> 22\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Keep track of the best model on the validation set\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m, in \u001b[0;36mtrain_and_eval\u001b[1;34m(model, learning_rate, momentum, weight_decay)\u001b[0m\n\u001b[0;32m     10\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, momentum\u001b[38;5;241m=\u001b[39mmomentum, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m---> 12\u001b[0m performance_sgd, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCH_COUNT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Accuracies ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 73\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch_count, optimizer, model, loss_function, train_loader, eval_mid_training)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mepoch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m performance \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy_train\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mcompute_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy_val\u001b[39m\u001b[38;5;124m'\u001b[39m: compute_accuracy(model, val_loader)\n\u001b[0;32m     75\u001b[0m }\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m performance, epoch_metrics\n",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m, in \u001b[0;36mcompute_accuracy\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m     12\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y_true \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     15\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;66;03m# Pick the class with the highest probability\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mats\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:627\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[0;32m    628\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mats\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\profiler.py:622\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 622\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mc:\\Users\\Mats\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_ops.py:513\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyper_parameters = [\n",
    "    {'learning_rate': .01, 'momentum': .0, 'weight_decay': .000},\n",
    "    {'learning_rate': .01, 'momentum': .0, 'weight_decay': .010},\n",
    "    {'learning_rate': .01, 'momentum': .9, 'weight_decay': .000},\n",
    "    {'learning_rate': .01, 'momentum': .9, 'weight_decay': .010},\n",
    "    {'learning_rate': .01, 'momentum': .9, 'weight_decay': .001},\n",
    "    {'learning_rate': .01, 'momentum': .8, 'weight_decay': .010},\n",
    "]\n",
    "\n",
    "print(\"Global parameters:\")\n",
    "print(\"Batch size: \", BATCH_SIZE)\n",
    "print(\"Epoch count: \", EPOCH_COUNT)\n",
    "print(\"Loss function: CrossEntropyLoss\")\n",
    "print(\"Seed: \", RANDOM_SEED)\n",
    "\n",
    "best_accuracy = -1\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for params in hyper_parameters:\n",
    "    model = MyMLP()\n",
    "    accuracy = train_and_eval(model, params['learning_rate'], params['momentum'], params['weight_decay'])\n",
    "\n",
    "    # Keep track of the best model on the validation set\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "        best_params = params\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Best parameters:\")\n",
    "print(f\"Learning rate: {best_params['leanring_rate']}\")\n",
    "print(f\"Momentum: {best_params['momentum']}\")\n",
    "print(f\"Weight decay: {best_params['weight_decay']}\")\n",
    "\n",
    "# Show accuracy on unseen test data\n",
    "test_accuracy = compute_accuracy(best_model, test_loader)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
