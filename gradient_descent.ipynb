{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "TRAIN_SPLIT_PERCENTAGE = 0.8\n",
    "VALIDATION_SPLIT_PERCENTAGE = 1 - TRAIN_SPLIT_PERCENTAGE\n",
    "RANDOM_SEED = 265\n",
    "EPOCH_COUNT = 30\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.ConvertImageDtype(torch.double),\n",
    "\n",
    "    # Normalize the pixel color values to be between -1 and 1\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dataset to 'airplane' and 'bird' only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "Included labels:  ['airplane', 'bird']\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_LABELS = [label for label in cifar10_test.classes]\n",
    "INCLUDED_LABELS = ['airplane', 'bird']\n",
    "\n",
    "print(\"Labels: \", CIFAR10_LABELS)\n",
    "print(\"Included labels: \", INCLUDED_LABELS)\n",
    "\n",
    "included_labels_indices = [i for i, label in enumerate(CIFAR10_LABELS) if label in INCLUDED_LABELS]\n",
    "\n",
    "cifar10_train_included_indices = [i for i, target in enumerate(cifar10_train.targets) if target in included_labels_indices]\n",
    "cifar10_test_included_indices = [i for i, target in enumerate(cifar10_test.targets) if target in included_labels_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and create loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8000 entries\n",
      "Validate: 2000 entries\n",
      "Test: 2000 entries\n"
     ]
    }
   ],
   "source": [
    "train_indices = random.sample(cifar10_train_included_indices, int(TRAIN_SPLIT_PERCENTAGE * len(cifar10_train_included_indices)))\n",
    "val_indices = [i for i in cifar10_train_included_indices if i not in train_indices]\n",
    "\n",
    "train_subset = torch.utils.data.Subset(cifar10_train, train_indices)\n",
    "val_subset = torch.utils.data.Subset(cifar10_train, val_indices)\n",
    "test_subset = torch.utils.data.Subset(cifar10_test, cifar10_test_included_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train: {len(train_subset)} entries\")\n",
    "print(f\"Validate: {len(val_indices)} entries\")\n",
    "print(f\"Test: {len(cifar10_test_included_indices)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the dataset\n",
    "I will now visualize and analyse the dataset.\n",
    "\n",
    "## Obvious bises?\n",
    "To identify obvious biases in the dataset, I need to visualize the distribution of labels.  \n",
    "If one label is grosely overrepresented, I might have to downsample the other label.\n",
    "\n",
    "## What does the data look like?\n",
    "The labels sais birds and planes, but I want to know excactly what I am working with.  \n",
    "Therefore I want to show one example of each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_data_example(image_tensor):\n",
    "    # Unnormalize the pixel color values\n",
    "    image_tensor = image_tensor * 0.5 + 0.5\n",
    "\n",
    "    np_image = image_tensor.numpy()\n",
    "    np_image = np.transpose(np_image, (1, 2, 0))\n",
    "    plt.imshow(np_image)\n",
    "    plt.show()\n",
    "\n",
    "def show_data_examples(classes, loader):\n",
    "    include_labels = [x for x in classes]\n",
    "    examples = []\n",
    "    for images, labels in loader:\n",
    "        if len(include_labels) == 0: continue\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            label = classes[labels[i].item()]\n",
    "            if label not in include_labels: continue\n",
    "            examples.append(images[i])\n",
    "            include_labels.remove(label)\n",
    "\n",
    "    show_data_example(torchvision.utils.make_grid(examples))\n",
    "\n",
    "def show_label_distribution(classes, loader):\n",
    "    distribution = {label: 0 for label in classes}\n",
    "    for _, labels in loader:\n",
    "        for label in labels:\n",
    "            label_name = classes[label.item()]\n",
    "            distribution[label_name] += 1\n",
    "\n",
    "    plt.bar(distribution.keys(), distribution.values())\n",
    "    plt.show()\n",
    "\n",
    "# show_label_distribution(included_labels, train_loader)\n",
    "# show_data_examples(included_labels, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.input = nn.Linear(3 * 32 * 32, 512)\n",
    "        self.hidden1 = nn.Linear(512, 128)\n",
    "        self.hidden2 = nn.Linear(128, 32)\n",
    "        self.hidden3 = nn.Linear(32, 2)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.activation(self.input(x))\n",
    "        x = self.activation(self.hidden1(x))\n",
    "        x = self.activation(self.hidden2(x))\n",
    "        x = self.hidden3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2264158472.py, line 63)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 63\u001b[1;36m\u001b[0m\n\u001b[1;33m    'epoch': epoch + 1\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "def map_labels_to_neurons(y_true):\n",
    "    return torch.Tensor([[1.0, 0.0] if i.item() == 0 else [0.0, 1.0] for i in y_true])\n",
    "\n",
    "def map_neurons_to_labels(y_pred):\n",
    "    # The prediction is based on included_labels,\n",
    "    # but the y_true has indecies from all classes\n",
    "    # Therefore we must map the prediction from included_labels to all classes\n",
    "    return torch.tensor([CIFAR10_LABELS.index(INCLUDED_LABELS[i.item()]) for i in y_pred])\n",
    "\n",
    "def compute_accuracy(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y_true in test_loader:\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # Pick the class with the highest probability\n",
    "            value, predicted = torch.max(y_pred.data, 1)\n",
    "            \n",
    "            predicted = map_neurons_to_labels(predicted)\n",
    "\n",
    "            # Update the score\n",
    "            total += y_true.size(0)\n",
    "            correct += (predicted == y_true).sum().item()\n",
    "\n",
    "    return correct / total, total, correct\n",
    "\n",
    "def train(epoch_count, optimizer, model, loss_function, train_loader, eval_mid_training=False):\n",
    "    epoch_metrics = []\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "\n",
    "        # We aggregate the loss to print after each epoch\n",
    "        # This is only for debugging purposes\n",
    "        epoch_loss = 0.0\n",
    "        epoch_size = 0\n",
    "\n",
    "        for X, y_true in train_loader:\n",
    "\n",
    "            # Gradient descent\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # Map the true predictions to match the \n",
    "            y_true = map_labels_to_neurons(y_true)\n",
    "\n",
    "            loss = loss_function(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_size += 1\n",
    "\n",
    "        # If we want metrics from training,\n",
    "        # we can return them here.\n",
    "        # NOTE: This will slow down the training significantly\n",
    "        if eval_mid_training:\n",
    "            train_accuracy, _, _ = compute_accuracy(model, train_loader)\n",
    "            val_accuracy, _, _ = compute_accuracy(model, val_loader)\n",
    "\n",
    "            # These might be used to create graphs\n",
    "            epoch_metrics.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'loss': epoch_loss / epoch_size,\n",
    "                'accuracy_train': train_accuracy,\n",
    "                'accuracy_val': val_accuracy\n",
    "            })\n",
    "\n",
    "        # Print loss after some epochs\n",
    "        # This is for debugging purposes\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch + 1} | Training loss: {epoch_loss / epoch_size}')\n",
    "\n",
    "    train_accuracy, _, _ = compute_accuracy(model, train_loader)\n",
    "    val_accuracy, _, _ = compute_accuracy(model, val_loader)\n",
    "    performance = {\n",
    "        'accuracy_train': train_accuracy,\n",
    "        'accuracy_val': val_accuracy\n",
    "    }\n",
    "\n",
    "    return performance, epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, learning_rate, momentum, weight_decay):\n",
    "    print(\"=========================================================\")\n",
    "    print(\"Current parameters:\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Momentum: {momentum}\")\n",
    "    print(f\"Weight decay: {weight_decay}\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"--------- Using Pytorch's SGD ---------\")\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    performance_sgd, _ = train(EPOCH_COUNT, optimizer, model, loss_function, train_loader)\n",
    "\n",
    "def train_and_eval_all():\n",
    "    hyper_parameters = [\n",
    "        {'learning_rate': .01, 'momentum': .0, 'weight_decay': .000},\n",
    "        {'learning_rate': .01, 'momentum': .0, 'weight_decay': .010},\n",
    "        {'learning_rate': .01, 'momentum': .9, 'weight_decay': .000},\n",
    "        {'learning_rate': .01, 'momentum': .9, 'weight_decay': .010},\n",
    "        {'learning_rate': .01, 'momentum': .9, 'weight_decay': .001},\n",
    "        {'learning_rate': .01, 'momentum': .8, 'weight_decay': .010},\n",
    "    ]\n",
    "\n",
    "    print(\"Global parameters:\")\n",
    "    print(\"Batch size: \", BATCH_SIZE)\n",
    "    print(\"Epoch count: \", EPOCH_COUNT)\n",
    "    print(\"Loss function: CrossEntropyLoss\")\n",
    "    print(\"Seed: \", RANDOM_SEED)\n",
    "\n",
    "    for params in hyper_parameters:\n",
    "        model = MyMLP()\n",
    "        train_and_eval(model, params['learning_rate'], params['momentum'], params['weight_decay'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
